{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKsjZRBKUWdO"
      },
      "source": [
        "# Set up the working environment\n",
        "We make use of the DeepRobust package from https://github.com/DSE-MSU/DeepRobust to perform attacks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhK247PHUiwJ"
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install deeprobust # check https://github.com/DSE-MSU/DeepRobust, provides easy\n",
        "# tools for attacking and training\n",
        "!pip install matplotlib\n",
        "%mkdir -p /content/csc413/project/\n",
        "%cd /content/csc413/project"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZtzaHJZuavM"
      },
      "source": [
        "# Access our human-label data sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbzdvboRyi_L",
        "outputId": "2cb1c25f-f753-4133-b899-8f5d2486e232"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# If access denied Login use xyzprojectuse@gmail.com to use this account's google drive\n",
        "# xyzprojectuse@gmail.com Csc413zxyandrew"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxn1Dp-buZ9_"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from  torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "#LOW RESOLUTION MNIST DATASET\n",
        "#################################################################################\n",
        "#This file creates the dataset object that pytorch uses to create batches.\n",
        "#Use this ModifiedMNISt instead of  datasets.MNIST when you want to train on the LOW resolution MNIST\n",
        "\n",
        "\n",
        "class Modified_MNIST(Dataset):\n",
        "    def __init__(self, train=True, resize=10):\n",
        "        %cd /content/gdrive/My Drive/datav2/\n",
        "        self.training = train\n",
        "\n",
        "        if train is True:\n",
        "\n",
        "            transform = torchvision.transforms.Compose([\n",
        "                                                        torchvision.transforms.Resize(resize, interpolation=InterpolationMode.BILINEAR),\n",
        "                                                        # Important to use InterpolationMode.NEAREST, to preserve blockiness\n",
        "                                                        torchvision.transforms.Resize(28, interpolation=InterpolationMode.NEAREST),\n",
        "                                                        torchvision.transforms.Lambda(lambda img: torchvision.transforms.functional.adjust_contrast(img, 4)),\n",
        "                                                        torchvision.transforms.ToTensor()])\n",
        "            self.dataset = torchvision.datasets.MNIST('./', train=train,\n",
        "                                                      download=False,\n",
        "                                                      transform=transform)\n",
        "        else:\n",
        "            transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "            self.dataset = torchvision.datasets.MNIST('./', train=train, download=False, transform=transform)\n",
        "\n",
        "\n",
        "        with open('train_labels.json', 'r') as fp:\n",
        "            self.labels_dict = json.load(fp)\n",
        "\n",
        "        with open('correct_indices.json', 'r') as fp:\n",
        "            self.correct_indices = json.load(fp)\n",
        "        %cd /content/csc413/project\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.training is True:\n",
        "            return len(self.correct_indices) #Return number of correctly labelled\n",
        "        else:\n",
        "            return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.training is True:\n",
        "            actual_index = self.correct_indices[idx] #actual_index corresponds to the index for the raw MNIST dataset\n",
        "            img = self.dataset[actual_index][0]\n",
        "            return (img, int(self.labels_dict[str(actual_index)][\"label\"]))\n",
        "        else:\n",
        "            img = self.dataset[idx][0]\n",
        "            original_label = self.dataset[idx][1]\n",
        "            return (img, original_label)\n",
        "\n",
        "\n",
        "class Limited_MNIST(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        self.training = train\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "        self.dataset = torchvision.datasets.MNIST('./', train=train, download=False, transform=transform)\n",
        "        with open('correct_indices.json', 'r') as fp:\n",
        "            self.correct_indices = json.load(fp)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.training is True:\n",
        "            return len(self.correct_indices) #Return number of correctly labelled\n",
        "        else:\n",
        "            return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.training is True:\n",
        "            actual_index = self.correct_indices[idx] #actual_index corresponds to the index for the raw MNIST dataset\n",
        "            img = self.dataset[actual_index][0]     #Here we use the original MNIST images\n",
        "            original_label = self.dataset[actual_index][1] #Here we use the original MNIST labels\n",
        "            return (img, original_label)\n",
        "        else:\n",
        "            img = self.dataset[idx][0]\n",
        "            original_label = self.dataset[idx][1]         #Here we use the original MNIST images\n",
        "            return (img, original_label)                #Here we use the original MNIST labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8L2lPYb1Agf"
      },
      "source": [
        "# Networks used\n",
        "The same version of CNN and LeNet is used for both datasets, only the in channel and other necessary dimensions are changed to make the training possible. Only MNIST is analysised due to time and resourses constraints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXkXEzof1J04"
      },
      "source": [
        "# CNN for MNIST\n",
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "class CNN_For_MNIST(nn.Module):\n",
        "    def __init__(self, in_channel1 = 1, out_channel1 = 32, out_channel2 = 64, H = 28, W = 28):\n",
        "        super(CNN_For_MNIST, self).__init__()\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "        self.out_channel2 = out_channel2\n",
        "\n",
        "        ## define two convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels = in_channel1,\n",
        "                               out_channels = out_channel1,\n",
        "                               kernel_size = 5,\n",
        "                               stride= 1,\n",
        "                               padding = (2,2))\n",
        "        self.conv2 = nn.Conv2d(in_channels = out_channel1,\n",
        "                               out_channels = out_channel2,\n",
        "                               kernel_size = 5,\n",
        "                               stride = 1,\n",
        "                               padding = (2,2))\n",
        "\n",
        "        ## define two linear layers\n",
        "        self.fc1 = nn.Linear(int(H/4)*int(W/4)* out_channel2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, int(self.H/4) * int(self.W/4) * self.out_channel2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def get_logits(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, int(self.H/4) * int(self.W/4) * self.out_channel2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4c0Gs1wYf2s"
      },
      "source": [
        "# LeNet for MNIST\n",
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "class LeNet_For_MNIST(Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet_For_MNIST, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(256, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y\n",
        "\n",
        "    def get_logits(self, x):\n",
        "      y = self.conv1(x)\n",
        "      y = self.relu1(y)\n",
        "      y = self.pool1(y)\n",
        "      y = self.conv2(y)\n",
        "      y = self.relu2(y)\n",
        "      y = self.pool2(y)\n",
        "      y = y.view(y.shape[0], -1)\n",
        "      y = self.fc1(y)\n",
        "      y = self.relu3(y)\n",
        "      y = self.fc2(y)\n",
        "      y = self.relu4(y)\n",
        "      y = self.fc3(y)\n",
        "      y = self.relu5(y)\n",
        "      return y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws1nL6jm1E6M"
      },
      "source": [
        "# CNN for CIFAR10\n",
        "class CNN_For_CIFAR(nn.Module):\n",
        "    def __init__(self, in_channel1 = 3, out_channel1 = 32, out_channel2 = 64, H = 32, W = 32):\n",
        "        super(CNN_For_CIFAR, self).__init__()\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "        self.out_channel2 = out_channel2\n",
        "\n",
        "        ## define two convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels = in_channel1,\n",
        "                               out_channels = out_channel1,\n",
        "                               kernel_size = 5,\n",
        "                               stride= 1,\n",
        "                               padding = (2,2))\n",
        "        self.conv2 = nn.Conv2d(in_channels = out_channel1,\n",
        "                               out_channels = out_channel2,\n",
        "                               kernel_size = 5,\n",
        "                               stride = 1,\n",
        "                               padding = (2,2))\n",
        "\n",
        "        ## define two linear layers\n",
        "        self.fc1 = nn.Linear(int(H/4)*int(W/4)* out_channel2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, int(self.H/4) * int(self.W/4) * self.out_channel2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmKAS9UX7Lf7"
      },
      "source": [
        "# LeNet for CIFAR10\n",
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "\n",
        "class LeNet_For_CIFAR(Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet_For_CIFAR, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFMozLNFq2fe"
      },
      "source": [
        "# Training\n",
        "The trained models are saved in the google drive and can be loadaded directly, so there is no need to re-train the models before doing attacks. It is sufficient to only run the Helper functions for loading trained model section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LCShsLp9cni"
      },
      "source": [
        "## Helper functions for training\n",
        "Do not need to expand, just run this section once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmDItOmN2-Ao"
      },
      "source": [
        "def model_train(model, device, train_loader, optimizer, epoch):\n",
        "    \"\"\"train network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model :\n",
        "        model\n",
        "    device :\n",
        "        device(option:'cpu','cuda')\n",
        "    train_loader :\n",
        "        training data loader\n",
        "    optimizer :\n",
        "        optimizer\n",
        "    epoch :\n",
        "        epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #print every 10\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def model_test(model, device, test_loader):\n",
        "    \"\"\"test network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model :\n",
        "        model\n",
        "    device :\n",
        "        device(option:'cpu', 'cuda')\n",
        "    test_loader :\n",
        "        testing data loader\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfDYEax0c1XN"
      },
      "source": [
        "# We make change to the training in DeepRobust, remove other model and add LeNet\n",
        "\"\"\"\n",
        "This function help to train model of different archtecture easily. \n",
        "Select model archtecture and training data, then output corresponding model.\n",
        "\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #233\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def modified_train(model, data, device, maxepoch, data_path = './', save_per_epoch = 10, seed = 100):\n",
        "    \"\"\"train.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model :\n",
        "        model(option:'CNN', 'LeNet')\n",
        "    data :\n",
        "        data(option:'MNIST','CIFAR10')\n",
        "    device :\n",
        "        device(option:'cpu', 'cuda')\n",
        "    maxepoch :\n",
        "        training epoch\n",
        "    data_path :\n",
        "        data path(default = './')\n",
        "    save_per_epoch :\n",
        "        save_per_epoch(default = 10)\n",
        "    seed :\n",
        "        seed\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    train_loader, test_loader = feed_dataset(data, data_path)\n",
        "\n",
        "    if (model == 'CNN'):\n",
        "        if (data == 'MNIST' or data == \"Limited_MNIST\" or data == \"Modified_MNIST\"):\n",
        "          train_net = CNN_For_MNIST().to(device)\n",
        "        else:\n",
        "          train_net = CNN_For_CIFAR().to(device)\n",
        "    elif (model == 'LeNet'):\n",
        "        if (data == 'MNIST' or data == \"Limited_MNIST\" or data == \"Modified_MNIST\"):\n",
        "          train_net = LeNet_For_MNIST().to(device)\n",
        "        else:\n",
        "          train_net = LeNet_For_CIFAR().to(device)\n",
        "\n",
        "    optimizer = optim.SGD(train_net.parameters(), lr= 0.1, momentum=0.5)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n",
        "    save_model = True\n",
        "    for epoch in range(1, maxepoch + 1):     ## 5 batches\n",
        "\n",
        "        print(epoch)\n",
        "        model_train(train_net, device, train_loader, optimizer, epoch)\n",
        "        model_test(train_net, device, test_loader)\n",
        "\n",
        "        if (save_model and (epoch % (save_per_epoch) == 0 or epoch == maxepoch)):\n",
        "            if os.path.isdir('./trained_models/'):\n",
        "                print('Save model.')\n",
        "                torch.save(train_net.state_dict(), './trained_models/'+ data + \"_\" + model + \"_epoch_\" + str(epoch) + \".pt\")\n",
        "            else:\n",
        "                os.mkdir('./trained_models/')\n",
        "                print('Make directory and save model.')\n",
        "                torch.save(train_net.state_dict(), './trained_models/'+ data + \"_\" + model + \"_epoch_\" + str(epoch) + \".pt\")\n",
        "        scheduler.step()\n",
        "\n",
        "def feed_dataset(data, data_dict):\n",
        "    if(data == 'CIFAR10'):\n",
        "        transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=5),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                ])\n",
        "\n",
        "        transform_val = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                ])\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "                 datasets.CIFAR10(data_dict, train=True, download = True,\n",
        "                        transform=transform_train),\n",
        "                 batch_size= 128, shuffle=True) #, **kwargs)\n",
        "\n",
        "        test_loader  = torch.utils.data.DataLoader(\n",
        "                 datasets.CIFAR10(data_dict, train=False, download = True,\n",
        "                        transform=transform_val),\n",
        "                batch_size= 1000, shuffle=True) #, **kwargs)\n",
        "\n",
        "    \n",
        "    elif(data == 'MNIST'):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "                 datasets.MNIST(data_dict, train=True, download = True,\n",
        "                 transform=transforms.Compose([transforms.ToTensor(),\n",
        "                 transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                 batch_size=128,\n",
        "                 shuffle=True)\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "                datasets.MNIST(data_dict, train=False, download = True,\n",
        "                transform=transforms.Compose([transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                batch_size=1000,\n",
        "                shuffle=True)\n",
        "\n",
        "    elif(data == 'Modified_MNIST'):\n",
        "        data_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        train_loader = torch.utils.data.DataLoader(Modified_MNIST(), \n",
        "        batch_size=128, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "                datasets.MNIST(data_dict, train=False, download = True,\n",
        "                transform=data_transform), batch_size=1000, shuffle=True)\n",
        "    \n",
        "    elif(data == 'Limited_MNIST'):\n",
        "        data_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        train_loader = torch.utils.data.DataLoader(Limited_MNIST(), \n",
        "        batch_size=128, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "                datasets.MNIST(data_dict, train=False, download = True,\n",
        "                transform=data_transform), batch_size=1000, shuffle=True)\n",
        "    \n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def fine_tune(model, data, device, train_net, maxepoch, data_path = './', save_per_epoch = 10, seed = 100):\n",
        "    \"\"\"Fine_tune the last layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model :\n",
        "        model(option:'CNN', 'LeNet')\n",
        "    data :\n",
        "        data(option:'MNIST','CIFAR10')\n",
        "    device :\n",
        "        device(option:'cpu', 'cuda')\n",
        "    maxepoch :\n",
        "        training epoch\n",
        "    data_path :\n",
        "        data path(default = './')\n",
        "    save_per_epoch :\n",
        "        save_per_epoch(default = 10)\n",
        "    seed :\n",
        "        seed\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    train_loader, test_loader = feed_dataset(data, data_path)\n",
        "\n",
        "    for param in train_net.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # Parameters of newly constructed modules have requires_grad=True by default\n",
        "    if model == \"CNN\":\n",
        "      #   CNN_For_MNIST\n",
        "      # (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "      # (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "      # (fc1): Linear(in_features=3136, out_features=1024, bias=True)\n",
        "      # (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
        "      # Last linear layer is fine-tuned\n",
        "      train_net.fc2 = nn.Linear(1024, 10)\n",
        "    else:\n",
        "        # LeNet_For_MNIST(\n",
        "        #   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
        "        #   (relu1): ReLU()\n",
        "        #   (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        #   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
        "        #   (relu2): ReLU()\n",
        "        #   (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        #   (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
        "        #   (relu3): ReLU()\n",
        "        #   (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
        "        #   (relu4): ReLU()\n",
        "        #   (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
        "        #   (relu5): ReLU()\n",
        "        # )\n",
        "      # train_net.conv2 = nn.Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
        "      train_net.fc3 = nn.Linear(84, 10)\n",
        "    \n",
        "    train_net = train_net.to(device)\n",
        "\n",
        "    train_para = []\n",
        "    for param in train_net.parameters():\n",
        "      if param.requires_grad == True:\n",
        "        train_para.append(param)\n",
        "\n",
        "    optimizer = optim.SGD(train_para, lr= 0.1, momentum=0.5)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n",
        "    save_model = True\n",
        "    for epoch in range(1, maxepoch + 1):     ## 5 batches\n",
        "\n",
        "        print(epoch)\n",
        "        model_train(train_net, device, train_loader, optimizer, epoch)\n",
        "        model_test(train_net, device, test_loader)\n",
        "\n",
        "        if (save_model and (epoch % (save_per_epoch) == 0 or epoch == maxepoch)):\n",
        "            if os.path.isdir('./trained_models/'):\n",
        "                print('Save model.')\n",
        "                torch.save(train_net.state_dict(), './trained_models/'+ data + \"_\" + model + \"_ft_epoch_\" + str(epoch) + \".pt\")\n",
        "            else:\n",
        "                os.mkdir('./trained_models/')\n",
        "                print('Make directory and save model.')\n",
        "                torch.save(train_net.state_dict(), './trained_models/'+ data + \"_\" + model + \"_ft_epoch_\" + str(epoch) + \".pt\")\n",
        "        scheduler.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Y3ozrIsFvN"
      },
      "source": [
        "## Helper functions for loading trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuhVR-gHKk3J"
      },
      "source": [
        "def load_model(network, dataset, cloud = True):\n",
        "  location = \"/content/csc413/project/trained_models/\"\n",
        "  if cloud: \n",
        "    location = \"/content/gdrive/MyDrive/trained_models/\"\n",
        "  model = CNN_For_MNIST()\n",
        "  if dataset == \"MNIST\" or dataset == \"Modified_MNIST\":\n",
        "    location = location + dataset + \"_\" + network + \"_epoch_20.pt\"\n",
        "    if network == \"LeNet\":\n",
        "      model = LeNet_For_MNIST()\n",
        "  else:\n",
        "    location = location + dataset + \"_\" + network + \"_epoch_20.pt\"\n",
        "    model = CNN_For_CIFAR()\n",
        "    if network == \"LeNet\":\n",
        "      model = LeNet_For_CIFAR()\n",
        "\n",
        "  model.load_state_dict(torch.load(location, map_location = torch.device('cuda')))\n",
        "  model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI45-120PZG2"
      },
      "source": [
        "def load_model_ft(network, dataset, cloud = True):\n",
        "  location = \"/content/csc413/project/trained_models/\"\n",
        "  if cloud: \n",
        "    location = \"/content/gdrive/MyDrive/trained_models/\"\n",
        "  model = CNN_For_MNIST()\n",
        "  if dataset == \"MNIST\" or dataset == \"Modified_MNIST\":\n",
        "    location = location + dataset + \"_\" + network + \"_ft_epoch_20.pt\"\n",
        "    if network == \"LeNet\":\n",
        "      model = LeNet_For_MNIST()\n",
        "  else:\n",
        "    location = location + dataset + \"_\" + network + \"_ft_epoch_20.pt\"\n",
        "    model = CNN_For_CIFAR()\n",
        "    if network == \"LeNet\":\n",
        "      model = LeNet_For_CIFAR()\n",
        "\n",
        "  model.load_state_dict(torch.load(location, map_location = torch.device('cuda')))\n",
        "  model.eval()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCEhWyjD9mo9"
      },
      "source": [
        "## Interface for trainning and loading dataset\n",
        "Only run this if you want to re-train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwKuE-EQswHm"
      },
      "source": [
        "modified_train('CNN', 'MNIST', 'cuda', 20)\n",
        "modified_train('CNN', 'Modified_MNIST', 'cuda', 20)\n",
        "fine_tune('CNN', 'Modified_MNIST', 'cuda', load_model('CNN', 'MNIST'), 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ-XBvBHtEj0"
      },
      "source": [
        "modified_train('LeNet', 'MNIST', 'cuda', 20)\n",
        "modified_train('LeNet', 'Modified_MNIST', 'cuda', 20)\n",
        "fine_tune('LeNet', 'Modified_MNIST', 'cuda', load_model('LeNet', 'MNIST'), 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se58otegtxvw"
      },
      "source": [
        "# If want to save the new models to drive\n",
        "# !cp -R '/content/csc413/project/trained_models' '/content/gdrive/My Drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBvDzx-Kp-1X"
      },
      "source": [
        "# Adversary attacking\n",
        "We use CW (https://arxiv.org/pdf/1608.04644.pdf) and fgsm (https://arxiv.org/pdf/1706.06083.pdf) as the attacking algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh4QVS0hKgPv"
      },
      "source": [
        "## Helper functions for attacking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ypneBLCOId4"
      },
      "source": [
        "def cw_attack(model, num_test):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  import random\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F #233\n",
        "  import torch.optim as optim\n",
        "  from torchvision import datasets,models,transforms\n",
        "  from PIL import Image\n",
        "\n",
        "  import logging\n",
        "\n",
        "  from deeprobust.image.attack.cw import CarliniWagner\n",
        "  from deeprobust.image.config import attack_params\n",
        "\n",
        "  # print log\n",
        "  logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "  logger = logging.getLogger(__name__)\n",
        "  logger.info(\"Start CW attack\")\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                datasets.MNIST('./', train=False, download = True,\n",
        "                transform=transforms.Compose([transforms.ToTensor()])),\n",
        "                batch_size=1,\n",
        "                shuffle=True)\n",
        "  \n",
        "  \n",
        "  device = \"cuda\"\n",
        "  batch_size = 1\n",
        "  batch_num = num_test\n",
        "  random_targeted = True\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  count = 0\n",
        "  classnum = 10\n",
        "  attackmethod = CarliniWagner(model, device='cuda')\n",
        "  \n",
        "  for count, (data, target) in enumerate(test_loader):\n",
        "      if count == batch_num:\n",
        "          break\n",
        "      print('batch:{}'.format(count))\n",
        "\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      \n",
        "      if(random_targeted == True):\n",
        "          r = list(range(0, target)) + list(range(target+1, classnum))\n",
        "          target_label = random.choice(r)\n",
        "          adv_example = attackmethod.generate(data, target, target_label = target_label, classnum = 10, **attack_params['CW_MNIST'])\n",
        "\n",
        "      elif(target_label >= 0):\n",
        "          adv_example = attackmethod.generate(data, target, target_label = target_label, classnum = 10, **attack_params['CW_MNIST'])\n",
        "\n",
        "\n",
        "      output = model(adv_example)\n",
        "  \n",
        "      test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "\n",
        "      pred = output.argmax(dim = 1, keepdim = True)  # get the index of the max log-probability.\n",
        "\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  batch_num = count+1\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(\"===== ACCURACY =====\")\n",
        "  print('Attack Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, batch_num * batch_size,\n",
        "        100. * correct / (batch_num * batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8l8tiyr9jmU"
      },
      "source": [
        "def fgsm(model, num_test):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  import random\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F #233\n",
        "  import torch.optim as optim\n",
        "  from torchvision import datasets,models,transforms\n",
        "  from PIL import Image\n",
        "\n",
        "  import logging\n",
        "\n",
        "  from deeprobust.image.attack.fgsm import FGSM\n",
        "  from deeprobust.image.config import attack_params\n",
        "\n",
        "  # print log\n",
        "  logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "  logger = logging.getLogger(__name__)\n",
        "  logger.info(\"Start FGSM attack\")\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "                datasets.MNIST('./', train=False, download = True,\n",
        "                transform=transforms.Compose([transforms.ToTensor()])),\n",
        "                batch_size=1,\n",
        "                shuffle=True)\n",
        "  \n",
        "  \n",
        "  device = \"cuda\"\n",
        "  batch_size = 1\n",
        "  batch_num = num_test\n",
        "  random_targeted = True\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  count = 0\n",
        "  classnum = 10\n",
        "  attackmethod = FGSM(model, device = \"cuda\")\n",
        "  \n",
        "  for count, (data, target) in enumerate(test_loader):\n",
        "      if count == batch_num:\n",
        "          break\n",
        "      print('batch:{}'.format(count))\n",
        "\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      \n",
        "      if(random_targeted == True):\n",
        "          r = list(range(0, target)) + list(range(target+1, classnum))\n",
        "          target_label = random.choice(r)\n",
        "          adv_example = attackmethod.generate(data, target, **attack_params['FGSM_MNIST'])\n",
        "\n",
        "      elif(target_label >= 0):\n",
        "          adv_example = attackmethod.generate(data, target, **attack_params['FGSM_MNIST'])\n",
        "\n",
        "\n",
        "      output = model(adv_example)\n",
        "  \n",
        "      test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "\n",
        "      pred = output.argmax(dim = 1, keepdim = True)  # get the index of the max log-probability.\n",
        "\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  batch_num = count+1\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(\"===== ACCURACY =====\")\n",
        "  print('Attack Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, batch_num * batch_size,\n",
        "        100. * correct / (batch_num * batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaNsR_5Vj1x2"
      },
      "source": [
        "## Interface for attacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Okn_IcaRlCo"
      },
      "source": [
        "model = load_model('CNN', 'MNIST')\n",
        "# model = load_model('LeNet', 'MNIST')\n",
        "# fgsm(model, 1000)\n",
        "# cw_attack(model, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d70plgldkPAE"
      },
      "source": [
        "model = load_model('CNN', 'Modified_MNIST')\n",
        "# model = load_model('LeNet', 'Modified_MNIST')\n",
        "# fgsm(model, 1000)\n",
        "cw_attack(model, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1B5B9WT_hga"
      },
      "source": [
        "model = load_model_ft('CNN', 'Modified_MNIST')\n",
        "# model = load_model_ft('LeNet', 'Modified_MNIST')\n",
        "# fgsm(model, 1000)\n",
        "cw_attack(model, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwUeulk3n94i"
      },
      "source": [
        "# Visualisation \n",
        "This provides the visualisation for conv layer and fc layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgzFMHt6oDDv"
      },
      "source": [
        "## Helper functions for visualisation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r4dpdbrn9Ov"
      },
      "source": [
        "def get_activation(name, activation):\n",
        "      def hook(model, input, output):\n",
        "          activation[name] = output.detach()\n",
        "      return hook\n",
        "\n",
        "def view_conv(model, layer='conv2'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  activation = {}\n",
        "  model.conv2.register_forward_hook(get_activation(layer, activation))\n",
        "  dataset=datasets.MNIST(\"/content/gdrive/MyDrive/\", transform=transforms.ToTensor(), download=False)\n",
        "  data, label = dataset[2]\n",
        "  #print(label)\n",
        "  data.unsqueeze_(0)\n",
        "  output = model(data)\n",
        "  act = activation[layer].squeeze()\n",
        "  col = int(act.size(0) ** 0.5) + 1\n",
        "  fig, axarr = plt.subplots(col, col, squeeze=False)\n",
        "  # print(act)\n",
        "  for idx in range(act.size(0)):\n",
        "    axarr[idx // col][idx % col].axis('off')\n",
        "    axarr[idx // col][idx % col].imshow(act[idx])\n",
        "\n",
        "  for idx in range(act.size(0), col * col):\n",
        "    fig.delaxes(axarr[idx // col][idx % col])\n",
        "\n",
        "\n",
        "def view_fc(model, layer='fc2'):\n",
        "  weights=model.fc2.weight.data\n",
        "  if layer == 'fc3':\n",
        "    weights=model.fc3.weight.data\n",
        "  col = int(weights.size(0) ** 0.5) + 1\n",
        "  n = int(weights[0].size(0) ** 0.5)\n",
        "  fig, axarr = plt.subplots(col, col)\n",
        "  for idx in range(weights.size(0)):\n",
        "    axarr[idx // col][idx % col].axis('off')\n",
        "    axarr[idx // col][idx % col].imshow(weights[idx].view(n,n))\n",
        "    \n",
        "  for idx in range(weights.size(0), col * col):\n",
        "    fig.delaxes(axarr[idx // col][idx % col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8tXiQcTumTa"
      },
      "source": [
        "def view_cw(model, data, target):\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F #233\n",
        "  import torch.optim as optim\n",
        "  from torchvision import datasets,models,transforms\n",
        "  from PIL import Image\n",
        "\n",
        "  import logging\n",
        "\n",
        "  from deeprobust.image.attack.cw import CarliniWagner\n",
        "  from deeprobust.image.netmodels.CNN import Net\n",
        "  from deeprobust.image.config import attack_params\n",
        "\n",
        "  # print log\n",
        "  logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "  logger = logging.getLogger(__name__)\n",
        "  logger.info(\"Start test cw attack\")\n",
        "\n",
        "  xx = datasets.MNIST('./', download = True).data[data]\n",
        "  xx = xx.unsqueeze_(0).float()/255\n",
        "  xx = xx.unsqueeze_(0).float().to('cuda')\n",
        "  X = xx.clone()\n",
        "\n",
        "  ## Set Target\n",
        "  yy = datasets.MNIST('/content/gdrive/MyDrive/', download = False).targets[data]\n",
        "  yy = yy.float()\n",
        "\n",
        "\n",
        "  attack = CarliniWagner(model, device='cuda')\n",
        "  AdvExArray = attack.generate(xx, yy, target_label = target, classnum = 10, **attack_params['CW_MNIST'])\n",
        "  Adv = AdvExArray.clone()\n",
        "\n",
        "  # test the result\n",
        "  predict0 = model(xx)\n",
        "  predict0= predict0.argmax(dim=1, keepdim=True)\n",
        "\n",
        "  # AdvExArray = torch.from_numpy(AdvExArray)\n",
        "  predict1 = model(Adv)\n",
        "  predict1= predict1.argmax(dim=1, keepdim=True)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  Adv = Adv.cpu()\n",
        "  X = X.cpu()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  #subplot(r,c) provide the no. of rows and columns\n",
        "  f, axarr = plt.subplots(1,2) \n",
        "  \n",
        "  print(\"========CW========\")\n",
        "  print(\"Original Prediction\")\n",
        "  print(predict0)\n",
        "  print(\"Prediction under CW Attack\")\n",
        "  print(predict1)\n",
        "  # use the created array to output your multiple images. In this case I have stacked 2 images vertically\n",
        "  axarr[0].imshow(X[0, 0]*255,cmap='gray',vmin=0,vmax=255)\n",
        "  axarr[1].imshow(Adv[0,0]*255,cmap='gray',vmin=0,vmax=255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtNIw7VKxRpW"
      },
      "source": [
        "def view_fgsm(model, data):\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F #233\n",
        "  import torch.optim as optim\n",
        "  from torchvision import datasets,models,transforms\n",
        "  from PIL import Image\n",
        "  import argparse\n",
        "\n",
        "  from deeprobust.image.attack.fgsm import FGSM\n",
        "  from deeprobust.image.netmodels.CNN import Net\n",
        "  from deeprobust.image.config import attack_params\n",
        "  from deeprobust.image.utils import download_model\n",
        "\n",
        "  xx = datasets.MNIST('/content/gdrive/MyDrive/', download = False).data[data:data + 1].to('cuda')\n",
        "  xx = xx.unsqueeze_(1).float()/255\n",
        "  yy = datasets.MNIST('/content/gdrive/MyDrive/', download = False).targets[data:data + 1].to('cuda')\n",
        "\n",
        "  \"\"\"\n",
        "  Generate adversarial examples\n",
        "  \"\"\"\n",
        "\n",
        "  F1 = FGSM(model, device = \"cuda\")       \n",
        "  AdvExArray = F1.generate(xx, yy, **attack_params['FGSM_MNIST'])\n",
        "\n",
        "  predict0 = model(xx)\n",
        "  predict0= predict0.argmax(dim=1, keepdim=True)\n",
        "\n",
        "  predict1 = model(AdvExArray)\n",
        "  predict1= predict1.argmax(dim=1, keepdim=True)\n",
        "\n",
        "  print(\"========FGSM========\")\n",
        "  print(\"FGSM original prediction:\")\n",
        "  print(predict0)\n",
        "\n",
        "  print(\"FGSM attack prediction:\")\n",
        "  print(predict1)\n",
        "\n",
        "  xx = xx.cpu().detach().numpy()\n",
        "  AdvExArray = AdvExArray.cpu().detach().numpy()\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  #subplot(r,c) provide the no. of rows and columns\n",
        "  f, axarr = plt.subplots(1,2) \n",
        "  # use the created array to output your multiple images. In this case I have stacked 2 images vertically\n",
        "  axarr[0].imshow(xx[0,0]*255,cmap='gray',vmin=0,vmax=255)\n",
        "  axarr[1].imshow(AdvExArray[0,0]*255,cmap='gray',vmin=0,vmax=255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHb8X5M7pzEz"
      },
      "source": [
        "## Interface for visualising"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b-oIHkoqGD3"
      },
      "source": [
        "#model = load_model('CNN', 'MNIST')\n",
        "#model = load_model('LeNet', 'MNIST')\n",
        "#view_conv(model)\n",
        "\n",
        "#model = load_model('CNN', 'Modified_MNIST')\n",
        "# view_conv(model)\n",
        "# view_fc(model)\n",
        "for i in range(5):\n",
        "  view_cw(model, i, target=9)\n",
        "  view_fgsm(model, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3QQvMg1qG_n"
      },
      "source": [
        "model = load_model_ft('CNN', 'Modified_MNIST')\n",
        "# model = load_model_ft('LeNet', 'Modified_MNIST')\n",
        "view_conv(model)\n",
        "view_fc(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if0xDFKfvnMC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN97lF7nvE3u"
      },
      "source": [
        "# Measure rubustness against number of pixels mutated\n",
        "To see how the models trained with different datasets perform differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wC7vbGlvS2p"
      },
      "source": [
        "## Helper functions for ploting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V56AFKoskO5W"
      },
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "from IPython.display import clear_output\n",
        "# test against distortion\n",
        "def distort(x, num_pixels=1, value=1.0):\n",
        "    for _ in range(num_pixels):\n",
        "        x[0][int(random()*28)][int(random()*28)] = value\n",
        "    return x\n",
        "\n",
        "def test_label_predictions(model, device, test_loader):\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            prediction = output.argmax(dim=1, keepdim=True)\n",
        "            actuals.extend(target.view_as(prediction))\n",
        "            predictions.extend(prediction)\n",
        "    return [i.item() for i in actuals], [i.item() for i in predictions]\n",
        "\n",
        "def accuracy_score(actuals, predictions):\n",
        "  a = np.array(actuals)\n",
        "  b = np.array(predictions)\n",
        "  return np.mean(a == b) \n",
        "\n",
        "def plot_accuracies(distorted_pixels, accuracies, model_names):\n",
        "    clear_output()\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.xlabel('Number of distorted pixels')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim((0.4, 1))\n",
        "    for i in range(len(model_names)):\n",
        "      plt.plot(distorted_pixels[i], accuracies[i], marker='o', label=model_names[i])\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYbxbWsZwJvr"
      },
      "source": [
        "def measure_acc():\n",
        "  model_names = ['CNN', \"CNN trained on Modified\", \"CNN fined tuned\"\n",
        "                , 'LeNet', \"LeNet trained on Modified\", \"LeNet fined tuned\"]\n",
        "  distorted_pixels_all = [[] for _ in range(len(model_names))]\n",
        "  accuracies_all = [[] for _ in range(len(model_names))]\n",
        "  models = [load_model('CNN', 'MNIST'), \n",
        "            load_model('CNN', 'Modified_MNIST'),\n",
        "            load_model_ft('CNN', 'Modified_MNIST'), \n",
        "            load_model('LeNet', 'MNIST'), \n",
        "            load_model('LeNet', 'Modified_MNIST'),\n",
        "            load_model_ft('LeNet', 'Modified_MNIST')]\n",
        "  for i in range(0, 20):\n",
        "      my_test_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST(\n",
        "              './',\n",
        "              train=False,\n",
        "              download=True,\n",
        "              transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda x: distort(x, num_pixels=i, value=5.0)),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "              ])\n",
        "          ),\n",
        "          batch_size=1000,\n",
        "          shuffle=True)\n",
        "      \n",
        "      for j in range(len(model_names)):\n",
        "        actuals, predictions = test_label_predictions(models[j], 'cuda', my_test_loader)\n",
        "        distorted_pixels_all[j].append(i)\n",
        "        accuracies_all[j].append(accuracy_score(actuals, predictions))\n",
        "      \n",
        "      plot_accuracies(distorted_pixels_all, accuracies_all, model_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCKiHthZwy1d"
      },
      "source": [
        "## Interface for ploting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxFGOAomwybW"
      },
      "source": [
        "measure_acc()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}